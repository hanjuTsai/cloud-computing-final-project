{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4b3e74cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.local import LocalSession\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"sagemaker/pytorch-health\"\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da8e1e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from packaging.version import Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b299a5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = sagemaker_session.upload_data(path='data', bucket=bucket, key_prefix=prefix)\n",
    "# print('input spec (in this case, just an S3 path): {}'.format(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8ed9d385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(entry_point='food101.py',\n",
    "                    role=role,\n",
    "                    py_version='py3',\n",
    "                    framework_version='1.8.0',\n",
    "                    instance_count=1,\n",
    "                    instance_type='ml.m5.xlarge',\n",
    "                    hyperparameters={\n",
    "                        'epochs': 1,\n",
    "                        'backend': 'gloo'\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "baf32f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_uri = \"s3://sagemaker-us-east-1-119266240050/sagemaker/pytorch-health\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b54c18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-10 18:37:22 Starting - Starting the training job...\n",
      "2022-05-10 18:37:46 Starting - Preparing the instances for trainingProfilerReport-1652207842: InProgress\n",
      ".........\n",
      "2022-05-10 18:39:14 Downloading - Downloading input data...........................................................................\n",
      "2022-05-10 18:51:57 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-05-10 18:51:59,477 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-05-10 18:51:59,480 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-05-10 18:51:59,492 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-05-10 18:51:59,500 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-05-10 18:52:00,028 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-05-10 18:52:00,048 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-05-10 18:52:00,066 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2022-05-10 18:52:00,079 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"backend\": \"gloo\",\n",
      "        \"epochs\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2022-05-10-18-37-21-520\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-119266240050/pytorch-training-2022-05-10-18-37-21-520/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"food101\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"food101.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"backend\":\"gloo\",\"epochs\":1}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=food101.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=food101\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-119266240050/pytorch-training-2022-05-10-18-37-21-520/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"backend\":\"gloo\",\"epochs\":1},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-05-10-18-37-21-520\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-119266240050/pytorch-training-2022-05-10-18-37-21-520/source/sourcedir.tar.gz\",\"module_name\":\"food101\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"food101.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--backend\",\"gloo\",\"--epochs\",\"1\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BACKEND=gloo\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 food101.py --backend gloo --epochs 1\u001b[0m\n",
      "\u001b[34mDistributed training - False\u001b[0m\n",
      "\u001b[34mNumber of gpus available - 0\u001b[0m\n",
      "\u001b[34mGet train data loader\u001b[0m\n",
      "\u001b[34m====================================================================================\u001b[0m\n",
      "\u001b[34mpath: /opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m/opt/ml/code\u001b[0m\n",
      "\u001b[34m====================================================================================\u001b[0m\n",
      "\u001b[34mGet test data loader\u001b[0m\n",
      "\u001b[34m====================================================================================\u001b[0m\n",
      "\u001b[34mpath: /opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34m/opt/ml/code\u001b[0m\n",
      "\u001b[34m====================================================================================\u001b[0m\n",
      "\u001b[34mProcesses 18937/18937 (100%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 6312/6312 (100%) of test data\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:04.036 algo-1:27 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:04.433 algo-1:27 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:04.433 algo-1:27 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:04.434 algo-1:27 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:04.434 algo-1:27 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:04.434 algo-1:27 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.088 algo-1:27 INFO hook.py:584] name:module.resnet.conv1.weight count_params:9408\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.088 algo-1:27 INFO hook.py:584] name:module.resnet.bn1.weight count_params:64\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.088 algo-1:27 INFO hook.py:584] name:module.resnet.bn1.bias count_params:64\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.088 algo-1:27 INFO hook.py:584] name:module.resnet.layer1.0.conv1.weight count_params:36864\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.088 algo-1:27 INFO hook.py:584] name:module.resnet.layer1.0.bn1.weight count_params:64\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.088 algo-1:27 INFO hook.py:584] name:module.resnet.layer1.0.bn1.bias count_params:64\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.088 algo-1:27 INFO hook.py:584] name:module.resnet.layer1.0.conv2.weight count_params:36864\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.088 algo-1:27 INFO hook.py:584] name:module.resnet.layer1.0.bn2.weight count_params:64\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.088 algo-1:27 INFO hook.py:584] name:module.resnet.layer1.0.bn2.bias count_params:64\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.088 algo-1:27 INFO hook.py:584] name:module.resnet.layer1.1.conv1.weight count_params:36864\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.088 algo-1:27 INFO hook.py:584] name:module.resnet.layer1.1.bn1.weight count_params:64\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.088 algo-1:27 INFO hook.py:584] name:module.resnet.layer1.1.bn1.bias count_params:64\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.088 algo-1:27 INFO hook.py:584] name:module.resnet.layer1.1.conv2.weight count_params:36864\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.088 algo-1:27 INFO hook.py:584] name:module.resnet.layer1.1.bn2.weight count_params:64\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.089 algo-1:27 INFO hook.py:584] name:module.resnet.layer1.1.bn2.bias count_params:64\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.089 algo-1:27 INFO hook.py:584] name:module.resnet.layer1.2.conv1.weight count_params:36864\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.089 algo-1:27 INFO hook.py:584] name:module.resnet.layer1.2.bn1.weight count_params:64\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.089 algo-1:27 INFO hook.py:584] name:module.resnet.layer1.2.bn1.bias count_params:64\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.089 algo-1:27 INFO hook.py:584] name:module.resnet.layer1.2.conv2.weight count_params:36864\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.089 algo-1:27 INFO hook.py:584] name:module.resnet.layer1.2.bn2.weight count_params:64\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.089 algo-1:27 INFO hook.py:584] name:module.resnet.layer1.2.bn2.bias count_params:64\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.089 algo-1:27 INFO hook.py:584] name:module.resnet.layer2.0.conv1.weight count_params:73728\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.089 algo-1:27 INFO hook.py:584] name:module.resnet.layer2.0.bn1.weight count_params:128\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.089 algo-1:27 INFO hook.py:584] name:module.resnet.layer2.0.bn1.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.089 algo-1:27 INFO hook.py:584] name:module.resnet.layer2.0.conv2.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.089 algo-1:27 INFO hook.py:584] name:module.resnet.layer2.0.bn2.weight count_params:128\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.089 algo-1:27 INFO hook.py:584] name:module.resnet.layer2.0.bn2.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.089 algo-1:27 INFO hook.py:584] name:module.resnet.layer2.0.downsample.0.weight count_params:8192\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.089 algo-1:27 INFO hook.py:584] name:module.resnet.layer2.0.downsample.1.weight count_params:128\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.089 algo-1:27 INFO hook.py:584] name:module.resnet.layer2.0.downsample.1.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.089 algo-1:27 INFO hook.py:584] name:module.resnet.layer2.1.conv1.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.089 algo-1:27 INFO hook.py:584] name:module.resnet.layer2.1.bn1.weight count_params:128\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.090 algo-1:27 INFO hook.py:584] name:module.resnet.layer2.1.bn1.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.090 algo-1:27 INFO hook.py:584] name:module.resnet.layer2.1.conv2.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.090 algo-1:27 INFO hook.py:584] name:module.resnet.layer2.1.bn2.weight count_params:128\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.090 algo-1:27 INFO hook.py:584] name:module.resnet.layer2.1.bn2.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.090 algo-1:27 INFO hook.py:584] name:module.resnet.layer2.2.conv1.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.090 algo-1:27 INFO hook.py:584] name:module.resnet.layer2.2.bn1.weight count_params:128\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.090 algo-1:27 INFO hook.py:584] name:module.resnet.layer2.2.bn1.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.090 algo-1:27 INFO hook.py:584] name:module.resnet.layer2.2.conv2.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.090 algo-1:27 INFO hook.py:584] name:module.resnet.layer2.2.bn2.weight count_params:128\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.090 algo-1:27 INFO hook.py:584] name:module.resnet.layer2.2.bn2.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.090 algo-1:27 INFO hook.py:584] name:module.resnet.layer2.3.conv1.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.090 algo-1:27 INFO hook.py:584] name:module.resnet.layer2.3.bn1.weight count_params:128\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.090 algo-1:27 INFO hook.py:584] name:module.resnet.layer2.3.bn1.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.090 algo-1:27 INFO hook.py:584] name:module.resnet.layer2.3.conv2.weight count_params:147456\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.090 algo-1:27 INFO hook.py:584] name:module.resnet.layer2.3.bn2.weight count_params:128\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.090 algo-1:27 INFO hook.py:584] name:module.resnet.layer2.3.bn2.bias count_params:128\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.090 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.0.conv1.weight count_params:294912\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.090 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.0.bn1.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.090 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.0.bn1.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.091 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.0.conv2.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.091 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.0.bn2.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.091 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.0.bn2.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.091 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.0.downsample.0.weight count_params:32768\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.091 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.0.downsample.1.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.091 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.0.downsample.1.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.091 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.1.conv1.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.091 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.1.bn1.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.091 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.1.bn1.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.091 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.1.conv2.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.091 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.1.bn2.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.091 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.1.bn2.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.091 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.2.conv1.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.091 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.2.bn1.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.091 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.2.bn1.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.091 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.2.conv2.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.092 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.2.bn2.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.092 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.2.bn2.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.092 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.3.conv1.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.092 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.3.bn1.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.092 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.3.bn1.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.092 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.3.conv2.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.092 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.3.bn2.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.092 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.3.bn2.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.092 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.4.conv1.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.092 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.4.bn1.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.092 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.4.bn1.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.092 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.4.conv2.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.092 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.4.bn2.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.092 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.4.bn2.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.092 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.5.conv1.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.092 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.5.bn1.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.092 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.5.bn1.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.092 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.5.conv2.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.092 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.5.bn2.weight count_params:256\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.092 algo-1:27 INFO hook.py:584] name:module.resnet.layer3.5.bn2.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.093 algo-1:27 INFO hook.py:584] name:module.resnet.layer4.0.conv1.weight count_params:1179648\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.093 algo-1:27 INFO hook.py:584] name:module.resnet.layer4.0.bn1.weight count_params:512\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.093 algo-1:27 INFO hook.py:584] name:module.resnet.layer4.0.bn1.bias count_params:512\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.093 algo-1:27 INFO hook.py:584] name:module.resnet.layer4.0.conv2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.093 algo-1:27 INFO hook.py:584] name:module.resnet.layer4.0.bn2.weight count_params:512\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.093 algo-1:27 INFO hook.py:584] name:module.resnet.layer4.0.bn2.bias count_params:512\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.093 algo-1:27 INFO hook.py:584] name:module.resnet.layer4.0.downsample.0.weight count_params:131072\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.093 algo-1:27 INFO hook.py:584] name:module.resnet.layer4.0.downsample.1.weight count_params:512\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.093 algo-1:27 INFO hook.py:584] name:module.resnet.layer4.0.downsample.1.bias count_params:512\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.093 algo-1:27 INFO hook.py:584] name:module.resnet.layer4.1.conv1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.093 algo-1:27 INFO hook.py:584] name:module.resnet.layer4.1.bn1.weight count_params:512\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.093 algo-1:27 INFO hook.py:584] name:module.resnet.layer4.1.bn1.bias count_params:512\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.093 algo-1:27 INFO hook.py:584] name:module.resnet.layer4.1.conv2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.093 algo-1:27 INFO hook.py:584] name:module.resnet.layer4.1.bn2.weight count_params:512\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.093 algo-1:27 INFO hook.py:584] name:module.resnet.layer4.1.bn2.bias count_params:512\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.093 algo-1:27 INFO hook.py:584] name:module.resnet.layer4.2.conv1.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.093 algo-1:27 INFO hook.py:584] name:module.resnet.layer4.2.bn1.weight count_params:512\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.093 algo-1:27 INFO hook.py:584] name:module.resnet.layer4.2.bn1.bias count_params:512\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.093 algo-1:27 INFO hook.py:584] name:module.resnet.layer4.2.conv2.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.094 algo-1:27 INFO hook.py:584] name:module.resnet.layer4.2.bn2.weight count_params:512\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.094 algo-1:27 INFO hook.py:584] name:module.resnet.layer4.2.bn2.bias count_params:512\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.094 algo-1:27 INFO hook.py:584] name:module.resnet.fc.weight count_params:512000\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.094 algo-1:27 INFO hook.py:584] name:module.resnet.fc.bias count_params:1000\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.094 algo-1:27 INFO hook.py:584] name:module.fc.weight count_params:101000\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.094 algo-1:27 INFO hook.py:584] name:module.fc.bias count_params:101\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.094 algo-1:27 INFO hook.py:586] Total Trainable Params: 21898773\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.094 algo-1:27 INFO hook.py:413] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-05-10 18:52:05.097 algo-1:27 INFO hook.py:476] Hook is writing from the hook with pid: 27\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [640/18937 (3%)] Loss: 2.873355\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [1280/18937 (7%)] Loss: 2.061604\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [1920/18937 (10%)] Loss: 1.870021\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [2560/18937 (14%)] Loss: 1.672549\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [3200/18937 (17%)] Loss: 1.517893\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [3840/18937 (20%)] Loss: 1.107311\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [4480/18937 (24%)] Loss: 1.143169\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [5120/18937 (27%)] Loss: 1.204691\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [5760/18937 (30%)] Loss: 1.206741\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [6400/18937 (34%)] Loss: 1.036487\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [7040/18937 (37%)] Loss: 1.232899\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [7680/18937 (41%)] Loss: 0.939363\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [8320/18937 (44%)] Loss: 0.894143\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [8960/18937 (47%)] Loss: 0.996856\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [9600/18937 (51%)] Loss: 0.700262\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [10240/18937 (54%)] Loss: 0.939506\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [10880/18937 (57%)] Loss: 0.680362\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [11520/18937 (61%)] Loss: 1.116823\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [12160/18937 (64%)] Loss: 1.160777\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [12800/18937 (68%)] Loss: 0.963811\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [13440/18937 (71%)] Loss: 0.824141\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [14080/18937 (74%)] Loss: 0.874177\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [14720/18937 (78%)] Loss: 0.642074\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [15360/18937 (81%)] Loss: 0.678360\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [16000/18937 (84%)] Loss: 0.678088\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [16640/18937 (88%)] Loss: 0.702579\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [17280/18937 (91%)] Loss: 0.769504\u001b[0m\n",
      "\n",
      "2022-05-10 20:13:59 Uploading - Uploading generated training model\u001b[34mTest set: Average loss: 0.6648, Accuracy: 4962/6312 (79%)\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34mDownloading: \"https://github.com/pytorch/vision/archive/v0.10.0.zip\" to /root/.cache/torch/hub/v0.10.0.zip\u001b[0m\n",
      "\u001b[34mDownloading: \"https://download.pytorch.org/models/resnet34-333f7ec4.pth\" to /root/.cache/torch/hub/checkpoints/resnet34-333f7ec4.pth\u001b[0m\n",
      "\u001b[34m#015  0%|          | 0.00/83.3M [00:00<?, ?B/s]#015 18%|█▊        | 15.1M/83.3M [00:00<00:00, 159MB/s]#015 36%|███▋      | 30.2M/83.3M [00:00<00:00, 155MB/s]#015 57%|█████▋    | 47.7M/83.3M [00:00<00:00, 168MB/s]#015 79%|███████▊  | 65.4M/83.3M [00:00<00:00, 173MB/s]#015100%|██████████| 83.3M/83.3M [00:00<00:00, 172MB/s]\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [640/18937 (3%)] Loss: 2.873355\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [1280/18937 (7%)] Loss: 2.061604\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [1920/18937 (10%)] Loss: 1.870021\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [2560/18937 (14%)] Loss: 1.672549\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [3200/18937 (17%)] Loss: 1.517893\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [3840/18937 (20%)] Loss: 1.107311\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [4480/18937 (24%)] Loss: 1.143169\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [5120/18937 (27%)] Loss: 1.204691\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [5760/18937 (30%)] Loss: 1.206741\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [6400/18937 (34%)] Loss: 1.036487\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [7040/18937 (37%)] Loss: 1.232899\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [7680/18937 (41%)] Loss: 0.939363\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [8320/18937 (44%)] Loss: 0.894143\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [8960/18937 (47%)] Loss: 0.996856\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [9600/18937 (51%)] Loss: 0.700262\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [10240/18937 (54%)] Loss: 0.939506\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [10880/18937 (57%)] Loss: 0.680362\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [11520/18937 (61%)] Loss: 1.116823\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [12160/18937 (64%)] Loss: 1.160777\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [12800/18937 (68%)] Loss: 0.963811\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [13440/18937 (71%)] Loss: 0.824141\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [14080/18937 (74%)] Loss: 0.874177\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [14720/18937 (78%)] Loss: 0.642074\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [15360/18937 (81%)] Loss: 0.678360\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [16000/18937 (84%)] Loss: 0.678088\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [16640/18937 (88%)] Loss: 0.702579\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [17280/18937 (91%)] Loss: 0.769504\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [17920/18937 (95%)] Loss: 0.926760\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [18560/18937 (98%)] Loss: 0.770777\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test set: Average loss: 0.6648, Accuracy: 4962/6312 (79%)\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving the model.\u001b[0m\n",
      "\u001b[34m2022-05-10 20:13:47,048 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-05-10 20:14:19 Completed - Training job completed\n",
      "Training seconds: 5716\n",
      "Billable seconds: 5716\n"
     ]
    }
   ],
   "source": [
    "estimator.fit({'training': data_uri})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "386c86eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ea0fb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "546832a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_train_data_loader(batch_size, training_dir, is_distributed, **kwargs):\n",
    "#     logger.info(\"Get train data loader\")\n",
    "    dataset = food101_custom(\n",
    "        training_dir,\n",
    "        'train',\n",
    "    )\n",
    "    train_sampler = (\n",
    "        torch.utils.data.distributed.DistributedSampler(dataset) if is_distributed else None\n",
    "    )\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=train_sampler is None,\n",
    "        sampler=train_sampler,\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a764c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class food101_custom(Dataset):\n",
    "    def __init__(self, path, partition, transform=None, target_transform=None):\n",
    "        \n",
    "        self.path = path.split(\"data\")[0]\n",
    "        print(self.path)\n",
    "        \n",
    "        with open(f'{path}/data/food-101/meta/{partition}.json', 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        \n",
    "        self.name2label = {name:i for i, name in enumerate(self.data.keys())}\n",
    "        self.label2name = {v:k for k,v in self.name2label.items()}\n",
    "        \n",
    "        self.data_list_form = []\n",
    "        for labelname, image_list in self.data.items():\n",
    "            for image_url in image_list:\n",
    "                self.data_list_form.append((image_url, labelname))\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list_form)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_path = os.path.join(self.path,'food-101','images', self.data_list_form[idx][0]+'.jpg')\n",
    "        image = Image.open(img_path)\n",
    "        image = transforms.ToTensor()(image)\n",
    "        image = transforms.Resize((224,224))(image)\n",
    "        \n",
    "        label = self.name2label[self.data_list_form[idx][1]]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d3646d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#import sagemaker_containers\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed94d22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='data/food-101/meta/train.json' mode='r' encoding='UTF-8'>\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/food-101/meta/train.json\", 'r') as f:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7240f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f100b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de70402ee3554f67868b801995d8c281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "649f9366cbc64e968f4312549f014270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee21ceeda3fb48a59c73c14555ccffb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902110cf9f624bc58e2d280c5845f0e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "           )"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.MNIST(\n",
    "    'data',\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32632703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "\n",
    "a = []\n",
    "for (dirpath, dirnames, filenames) in walk('data/food-101/images/'):\n",
    "    folders = dirnames\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84a1d4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3734c8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#import sagemaker_containers\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.utils.data.distributed\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "class food101_custom(Dataset):\n",
    "    def __init__(self, path, partition, transform=None, target_transform=None):\n",
    "        \n",
    "#         logger.info(\"====================================================================================\")\n",
    "#         logger.info(f\"path: {path}\")\n",
    "#         os.system(\"pwd\")\n",
    "#         logger.info(\"====================================================================================\")\n",
    "        \n",
    "        \n",
    "        self.path = path\n",
    "        \n",
    "        with open(os.path.join(self.path,'food-101','meta',f'{partition}.json'), 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "        \n",
    "        self.name2label = {name:i for i, name in enumerate(self.data.keys())}\n",
    "        self.label2name = {v:k for k,v in self.name2label.items()}\n",
    "        \n",
    "        self.data_list_form = []\n",
    "        for labelname, image_list in self.data.items():\n",
    "            for image_url in image_list:\n",
    "                self.data_list_form.append((image_url, labelname))\n",
    "\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list_form)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        success = False\n",
    "        \n",
    "        while not success:\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                preprocess = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Resize(256),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ])\n",
    "                \n",
    "                img_path = os.path.join(self.path,'food-101','images', self.data_list_form[idx][0]+'.jpg')\n",
    "                image = Image.open(img_path)\n",
    "                \n",
    "                preprocess = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Resize(256),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ])\n",
    "                \n",
    "                image = preprocess(image)\n",
    "            \n",
    "                if tuple(image.shape) != (3, 224, 224):\n",
    "                    print(tuple(image.shape))\n",
    "                    print(self.data_list_form[idx])\n",
    "                    print()\n",
    "                    idx = (idx+1) % len(self.data_list_form)\n",
    "                    continue\n",
    "                \n",
    "                success = True\n",
    "                \n",
    "            except:\n",
    "                idx = (idx+1) % len(self.data_list_form)\n",
    "        \n",
    "        label = self.name2label[self.data_list_form[idx][1]]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label\n",
    "    \n",
    "def _get_train_data_loader(batch_size, training_dir, is_distributed, **kwargs):\n",
    "#     logger.info(\"Get train data loader\")\n",
    "    dataset = food101_custom(\n",
    "        training_dir,\n",
    "        'train',\n",
    "    )\n",
    "    train_sampler = (\n",
    "        torch.utils.data.distributed.DistributedSampler(dataset) if is_distributed else None\n",
    "    )\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=train_sampler is None,\n",
    "        sampler=train_sampler,\n",
    "        **kwargs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "130ca7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = _get_train_data_loader(512, 'data', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f31b1a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for X, y in data_loader:\n",
    "    print(X.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2e4036",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuple(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4fa89fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7fd4acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://s3-media2.fl.yelpcdn.com/bphoto/dxMzmO9yAx5uWwiqzwjXCw/o.jpg\"\n",
    "im = Image.open(requests.get(url, stream=True).raw).resize((224,224))\n",
    "im = np.array(im).astype(np.float32)\n",
    "im = np.moveaxis(im, 2, 0)\n",
    "im = np.expand_dims(im, axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "20e5e3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = im/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1918a7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 224, 224)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "fbc5c2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = predictor.predict(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6cec8b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 101)"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "5bede247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.64702606e+00, -2.66228504e+01, -1.70597916e+01, -1.32028160e+01,\n",
       "       -3.99878464e+01, -4.67819214e+00, -5.31282043e+00, -1.46925697e+01,\n",
       "       -3.82949829e+01, -1.78865509e+01, -2.18997231e+01, -1.54518661e+01,\n",
       "       -2.49958534e+01, -2.10971832e+01, -2.78014679e+01, -1.98006115e+01,\n",
       "       -1.59833908e+01, -2.42354546e+01, -1.43897285e+01, -4.39290161e+01,\n",
       "       -1.19977341e+01, -2.95379829e+01, -3.38007088e+01, -1.89820658e-02,\n",
       "       -5.42748260e+00, -2.44128914e+01, -5.80835571e+01, -6.77180099e+01,\n",
       "       -5.48009491e+01, -5.58306503e+01, -6.47310638e+01, -5.68427544e+01,\n",
       "       -5.84883347e+01, -5.93338356e+01, -6.06045723e+01, -5.44488602e+01,\n",
       "       -5.07026215e+01, -5.99098625e+01, -5.06388702e+01, -5.08390732e+01,\n",
       "       -6.17877350e+01, -6.49610138e+01, -5.49770470e+01, -4.98209496e+01,\n",
       "       -5.58120079e+01, -5.92909241e+01, -5.13771286e+01, -6.18253517e+01,\n",
       "       -6.14178581e+01, -5.66344452e+01, -5.26415863e+01, -6.90209961e+01,\n",
       "       -5.67531853e+01, -5.64981728e+01, -5.43978195e+01, -7.16687012e+01,\n",
       "       -4.83160133e+01, -6.37096100e+01, -5.89683952e+01, -5.75845184e+01,\n",
       "       -5.70735054e+01, -6.07726212e+01, -5.30191879e+01, -6.19062576e+01,\n",
       "       -5.94247437e+01, -4.95305252e+01, -6.25491333e+01, -6.37256088e+01,\n",
       "       -5.44903793e+01, -5.98986435e+01, -5.07848091e+01, -5.35104065e+01,\n",
       "       -5.82755547e+01, -5.41035233e+01, -5.87704849e+01, -6.47184601e+01,\n",
       "       -5.40701141e+01, -6.22715225e+01, -5.70052834e+01, -5.99745750e+01,\n",
       "       -5.68379936e+01, -6.61865082e+01, -4.89962807e+01, -5.67022858e+01,\n",
       "       -5.98922272e+01, -5.08457794e+01, -5.71986351e+01, -5.47619362e+01,\n",
       "       -4.94484901e+01, -6.07583046e+01, -5.89744949e+01, -5.68823776e+01,\n",
       "       -5.94555931e+01, -5.09054680e+01, -4.66811523e+01, -5.52587090e+01,\n",
       "       -7.35929260e+01, -5.97593575e+01, -5.42048683e+01, -5.13226814e+01,\n",
       "       -5.46068802e+01])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "e0fcf809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "63052e8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(softmax(response[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "9579a8d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98119704262086"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(response[0])[23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6915e7bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_pytorch_latest_p37",
   "language": "python",
   "name": "conda_amazonei_pytorch_latest_p37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
